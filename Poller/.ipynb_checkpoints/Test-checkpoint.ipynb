{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import pickle\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "def saveClassifier():\n",
    "    save_class = open(\"classifier.pickle\", \"wb\")\n",
    "    pickle.dump(custom_sent_tokenizer, save_class)\n",
    "    save_class.close()\n",
    "    \n",
    "\n",
    "#saveClassifier()\n",
    "#process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "def findName(line):\n",
    "    \n",
    "    train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "    \n",
    "    custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "    \n",
    "    #text = inputFile.read()\n",
    "    \n",
    "    # tokenizing the input file (the main file)\n",
    "    tokenized = custom_sent_tokenizer.tokenize(line)[0]\n",
    "    #print(tokenized)\n",
    "    \n",
    "    # finding the names\n",
    "    try:\n",
    "        words = nltk.word_tokenize(tokenized)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "        #print(namedEnt)\n",
    "        #print(\"Broken down: \")\n",
    "        # determining if there's a name in the title\n",
    "        # that's such a broad ass descpritor (NNP)\n",
    "        for ent in namedEnt:\n",
    "            for entType in ent:\n",
    "                print(entType)\n",
    "                if entType[1] == \"NNP\":\n",
    "                    print(\"Found:\")\n",
    "                    print(line)\n",
    "                    print(entType[0])\n",
    "                    print(\"\\n\\n\")\n",
    "    except:\n",
    "        print(\"Error with tokenizing\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Starting...\")\n",
    "    \n",
    "inputFile = open(\"MainList-Copy1.txt\", \"r\")\n",
    "\n",
    "inputFile.seek(0, os.SEEK_SET)\n",
    "\n",
    "line = inputFile.readline()\n",
    "\n",
    "print(\"Finding name...\")\n",
    "while line:\n",
    "\n",
    "    #print(line)\n",
    "\n",
    "    if findName(line):\n",
    "        # if a name is found in the title, write it out to the new file\n",
    "        titleLine = line\n",
    "        urlLine = inputFile.readline()\n",
    "    else:\n",
    "        # otherwise, go the next title\n",
    "        line = inputFile.readline()\n",
    "    \n",
    "    line = inputFile.readline()\n",
    "\n",
    "inputFile.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "\n",
    "def findName(line):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    #doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "    doc = nlp(line)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        #print(ent.text, ent.label_)\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if ent.lemma_:\n",
    "                print(ent.lemma_)\n",
    "            print(\"\\t\\tPeRsOn\")\n",
    "        elif ent.label_ == \"ORG\" or ent.label_ == \"WORK_OF_ART\":\n",
    "            if ent.lemma_:\n",
    "                print(ent.lemma_)\n",
    "            print(\"\\t\\toRg Or WoRk Of ArT\")\n",
    "    \n",
    "        \n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "print(\"\\t\\t\\tStarting...\")\n",
    "    \n",
    "inputFile = open(\"MainList-Copy1.txt\", \"r\")\n",
    "\n",
    "inputFile.seek(0, os.SEEK_SET)\n",
    "\n",
    "line = inputFile.readline()\n",
    "\n",
    "print(\"\\t\\tFinding name...\")\n",
    "while line:\n",
    "\n",
    "    print('\\t', line)\n",
    "\n",
    "    findName(line)\n",
    "    line = inputFile.readline()\n",
    "    line = inputFile.readline()\n",
    "\n",
    "inputFile.close()\n",
    "\n",
    "print(\"\\t\\t\\tDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger as NERTagger\n",
    "\n",
    "def findName(line):\n",
    "    st = NERTagger('../poli_stanford_ner/stanford_ner/english.all.3class.distsim.crf.ser.gz', '../poli_stanford_ner/stanford_ner/stanford-ner-4.2.0.jar')\n",
    "    \n",
    "    pos = 0\n",
    "    savedPos = -1\n",
    "    multi_name = {}\n",
    "    ret_names = []\n",
    "    \n",
    "    # classifying if there are names in the sentence\n",
    "    for sent in nltk.sent_tokenize(line):\n",
    "        tokens = nltk.tokenize.word_tokenize(sent)\n",
    "        tags = st.tag(tokens)\n",
    "        for tag in tags:\n",
    "            if tag[1]=='PERSON': \n",
    "                print(tag)\n",
    "                multi_name[pos] = tag\n",
    "            pos += 1\n",
    "    # where it starts to see if there's first, middle, and last names\n",
    "    keys = isConsecutive(multi_name)\n",
    "    if keys:\n",
    "        #print(\"Multi name!\")\n",
    "        for keySet in keys:\n",
    "            tmp = None\n",
    "            for key in keySet:\n",
    "                if tmp is None:\n",
    "                    tmp = multi_name[key][0]\n",
    "                else:\n",
    "                    tmp += \"_\" + multi_name[key][0]\n",
    "            #print(\"\\t\\t\", tmp)\n",
    "            ret_names.append(tmp)\n",
    "    else:\n",
    "        tmp = None\n",
    "        for posInLine in multi_name:\n",
    "            # if this is the first time through\n",
    "            if savedPos == -1:\n",
    "                savedPos = posInLine\n",
    "            if savedPos+1 != posInLine:\n",
    "                tmp = multi_name[savedPos][0]\n",
    "                ret_names.append(tmp)\n",
    "            savedPos = posInLine\n",
    "    print(ret_names)\n",
    "    return ret_names\n",
    "    \n",
    "\n",
    "'''\n",
    "Checks if the given dictionary's keys are in (at least partial) sequential order\n",
    "Must use numeric keys for this \n",
    "'''\n",
    "def isConsecutive(d):\n",
    "    if len(d) <= 1:\n",
    "        return\n",
    "    \n",
    "    ret_keys = []\n",
    "    curr_keys = []\n",
    "    \n",
    "    keys = list(d.keys())\n",
    "    #print(keys, len(keys))\n",
    "    for i in range(len(keys)):\n",
    "        try:\n",
    "            # if the keys are sequential add them to the list\n",
    "            if keys[i+1] - keys[i] == 1:\n",
    "                #print(\"Coolio\")\n",
    "                curr_keys.append(keys[i])\n",
    "                curr_keys.append(keys[i+1])\n",
    "            else:\n",
    "                # if they're not, check if there's anything in the current list\n",
    "                if curr_keys:\n",
    "                    #print(\"\\t\\tBALLIN\")\n",
    "                    # if there is, remove any duplicates, sort it, \n",
    "                    # add to return keys, and clear the current list\n",
    "                    tmp = list(set(curr_keys))\n",
    "                    tmp.sort()\n",
    "                    ret_keys.append(tmp)\n",
    "                    curr_keys.clear()\n",
    "        except:\n",
    "            if i == len(keys):\n",
    "                if curr_keys:\n",
    "                    #print(\"\\t\\t\\tBALLING pt2\")\n",
    "                    # if there is, remove any duplicates, sort it, \n",
    "                    # add to return keys, and clear the current list\n",
    "                    tmp = list(set(curr_keys))\n",
    "                    tmp.sort()\n",
    "                    ret_keys.append(tmp)\n",
    "                    curr_keys.clear()\n",
    "            else:\n",
    "                print(\"\\t\\t\\t\\t\\tOUT OF BOUNDS\")\n",
    "                \n",
    "            if i == len(keys)-1:\n",
    "                #print(\"\\tAt boundary\")\n",
    "                tmp = list(set(curr_keys))\n",
    "                tmp.sort()\n",
    "                #print(tmp)\n",
    "                ret_keys.append(tmp)\n",
    "    \n",
    "    try:\n",
    "        if ret_keys[0]:\n",
    "            print(ret_keys)\n",
    "            return(ret_keys)\n",
    "        else:\n",
    "            #print('\\t', keys)\n",
    "            for key in keys:\n",
    "                tmpList = []\n",
    "                #print('\\t\\t', key)\n",
    "                tmpList.append(key)\n",
    "                ret_keys.append(tmpList)\n",
    "            if not ret_keys[0]:\n",
    "                del ret_keys[0]\n",
    "            print(ret_keys)\n",
    "            return ret_keys\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"\\t\\t\\tStarting...\")\n",
    "    \n",
    "inputFile = open(\"MainList-Copy1.txt\", \"r\")\n",
    "\n",
    "inputFile.seek(0, os.SEEK_SET)\n",
    "\n",
    "line = inputFile.readline()\n",
    "\n",
    "print(\"\\t\\tFinding names...\\n\")\n",
    "while line:\n",
    "\n",
    "    print('\\t', line)\n",
    "\n",
    "    names = findName(line)\n",
    "    print(names)\n",
    "    line = inputFile.readline()\n",
    "    line = inputFile.readline()\n",
    "    \n",
    "    print(\"----------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "inputFile.close()\n",
    "\n",
    "print(\"\\t\\t\\tDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
