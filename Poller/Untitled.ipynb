{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import pickle\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "def saveClassifier():\n",
    "    save_class = open(\"classifier.pickle\", \"wb\")\n",
    "    pickle.dump(custom_sent_tokenizer, save_class)\n",
    "    save_class.close()\n",
    "    \n",
    "\n",
    "#saveClassifier()\n",
    "#process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "def findName(line):\n",
    "    \n",
    "    train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "    \n",
    "    custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "    \n",
    "    #text = inputFile.read()\n",
    "    \n",
    "    # tokenizing the input file (the main file)\n",
    "    tokenized = custom_sent_tokenizer.tokenize(line)[0]\n",
    "    #print(tokenized)\n",
    "    \n",
    "    # finding the names\n",
    "    try:\n",
    "        words = nltk.word_tokenize(tokenized)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "        #print(namedEnt)\n",
    "        #print(\"Broken down: \")\n",
    "        # determining if there's a name in the title\n",
    "        # that's such a broad ass descpritor (NNP)\n",
    "        for ent in namedEnt:\n",
    "            for entType in ent:\n",
    "                print(entType)\n",
    "                if entType[1] == \"NNP\":\n",
    "                    print(\"Found:\")\n",
    "                    print(line)\n",
    "                    print(entType[0])\n",
    "                    print(\"\\n\\n\")\n",
    "    except:\n",
    "        print(\"Error with tokenizing\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Starting...\")\n",
    "    \n",
    "inputFile = open(\"MainList-Copy1.txt\", \"r\")\n",
    "\n",
    "inputFile.seek(0, os.SEEK_SET)\n",
    "\n",
    "line = inputFile.readline()\n",
    "\n",
    "print(\"Finding name...\")\n",
    "while line:\n",
    "\n",
    "    #print(line)\n",
    "\n",
    "    if findName(line):\n",
    "        # if a name is found in the title, write it out to the new file\n",
    "        titleLine = line\n",
    "        urlLine = inputFile.readline()\n",
    "    else:\n",
    "        # otherwise, go the next title\n",
    "        line = inputFile.readline()\n",
    "    \n",
    "    line = inputFile.readline()\n",
    "\n",
    "inputFile.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "\n",
    "def findName(line):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    #doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "    doc = nlp(line)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        #print(ent.text, ent.label_)\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if ent.lemma_:\n",
    "                print(ent.lemma_)\n",
    "            print(\"\\t\\tPeRsOn\")\n",
    "        elif ent.label_ == \"ORG\" or ent.label_ == \"WORK_OF_ART\":\n",
    "            if ent.lemma_:\n",
    "                print(ent.lemma_)\n",
    "            print(\"\\t\\toRg Or WoRk Of ArT\")\n",
    "    \n",
    "        \n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "print(\"\\t\\t\\tStarting...\")\n",
    "    \n",
    "inputFile = open(\"MainList-Copy1.txt\", \"r\")\n",
    "\n",
    "inputFile.seek(0, os.SEEK_SET)\n",
    "\n",
    "line = inputFile.readline()\n",
    "\n",
    "print(\"\\t\\tFinding name...\")\n",
    "while line:\n",
    "\n",
    "    print('\\t', line)\n",
    "\n",
    "    findName(line)\n",
    "    line = inputFile.readline()\n",
    "    line = inputFile.readline()\n",
    "\n",
    "inputFile.close()\n",
    "\n",
    "print(\"\\t\\t\\tDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger as NERTagger\n",
    "\n",
    "def findName(line):\n",
    "    st = NERTagger('../poli_stanford_ner/stanford_ner/english.all.3class.distsim.crf.ser.gz', '../poli_stanford_ner/stanford_ner/stanford-ner-4.2.0.jar')\n",
    "\n",
    "    #print(st.tag('You can call me Billiy Bubu and I live in Amsterdam.'.split()))\n",
    "\n",
    "    #text = \"\"\"You can call me Billiy Bubu and I live in Amsterdam.\"\"\"\n",
    "    text = line\n",
    "\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        tokens = nltk.tokenize.word_tokenize(sent)\n",
    "        tags = st.tag(tokens)\n",
    "        for tag in tags:\n",
    "            if tag[1]=='PERSON': \n",
    "                print(tag)\n",
    "\n",
    "print(\"\\t\\t\\tStarting...\")\n",
    "    \n",
    "inputFile = open(\"MainList-Copy1.txt\", \"r\")\n",
    "\n",
    "inputFile.seek(0, os.SEEK_SET)\n",
    "\n",
    "line = inputFile.readline()\n",
    "\n",
    "print(\"\\t\\tFinding name...\")\n",
    "while line:\n",
    "\n",
    "    print('\\t', line)\n",
    "\n",
    "    findName(line)\n",
    "    line = inputFile.readline()\n",
    "    line = inputFile.readline()\n",
    "\n",
    "inputFile.close()\n",
    "\n",
    "print(\"\\t\\t\\tDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
