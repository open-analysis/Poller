{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python3\n",
    "# CNN_sitemap_test       goes through CNN's sitemap to gather political pagesZ\n",
    "\n",
    "import requests, os, bs4, threading\n",
    "\n",
    "site = 'cnn'\n",
    "os.makedirs(site, exist_ok=True)    # store comics in ./xkcd\n",
    "\n",
    "\n",
    "def downloadPage():\n",
    "    url = 'https://cnn.com'\n",
    "    # Download the page.\n",
    "    print('Downloading page https://cnn.com/...')\n",
    "    res = requests.get(url+'/sitemap.html')\n",
    "    res.raise_for_status()\n",
    "\n",
    "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    article_endings = []\n",
    "    special_endings = []\n",
    "    \n",
    "    dateElem = soup.select('.date')\n",
    "    for elem in dateElem:\n",
    "        ref = elem.find('a')\n",
    "        href = ref.get('href')\n",
    "        # to only get articles\n",
    "        if href[1] is 'a':\n",
    "            #print(href)\n",
    "            article_endings.append(href)\n",
    "        # to get special\n",
    "        elif href[1] is 's':\n",
    "            if href[2] is 'p':\n",
    "                #print(href)\n",
    "                special_endings.append(href)\n",
    "    \n",
    "    url_endings = [article_endings, special_endings]\n",
    "    #print(url_endings)\n",
    "    #print(article_endings)\n",
    "    #print(special_endings)\n",
    "    \n",
    "    month_endings = []\n",
    "    \n",
    "    print(\"Checking yearly sitemaps\")\n",
    "    for endings in url_endings:\n",
    "        print(\"Checking specific sitemaps\")\n",
    "        for end in article_endings:        \n",
    "            res = requests.get(url+end)\n",
    "            res.raise_for_status()\n",
    "\n",
    "            soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "            monthElem = soup.select('.month')\n",
    "\n",
    "            for month in monthElem:\n",
    "                ref = month.find('a')\n",
    "                if ref is None:\n",
    "                    continue\n",
    "                href = ref.get('href')\n",
    "                #print(href)\n",
    "                month_endings.append(href)\n",
    "    \n",
    "    #print(month_endings)\n",
    "    final_urls = []\n",
    "    \n",
    "    print(\"Checking monthly entries\")\n",
    "    for month in month_endings:\n",
    "        print('\\t' + url + month)\n",
    "        res = requests.get(url+month)\n",
    "        res.raise_for_status()\n",
    "        \n",
    "        soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "        entryElem = soup.select('.sitemap-link')\n",
    "        #entryElem = soup.find_all('a')\n",
    "        #print(entryElem[0])\n",
    "        #print(entryElem)\n",
    "\n",
    "\n",
    "        for entry in entryElem:\n",
    "            if entry.string == 'Title':\n",
    "                continue\n",
    "            #print(entry)\n",
    "            ref = entry.find('a')\n",
    "            #print(ref)\n",
    "            if ref is None:\n",
    "                continue\n",
    "            href = ref.get('href')\n",
    "            #print(href)\n",
    "            final_urls.append(href)\n",
    "        \n",
    "    # save the urls for later\n",
    "    print ('Writing urls down...')\n",
    "    pageFile = open(os.path.join('cnn', 'urls.txt'), 'w')\n",
    "    for line in final_urls:\n",
    "        pageFile.write(line)\n",
    "        pageFile.write('\\n')\n",
    "\n",
    "    pageFile.close()\n",
    "            \n",
    "'''\n",
    "# Create and start the Thread objects.\n",
    "pageThreads = []             # a list of all the Thread objects\n",
    "for i in range(0, 140, 10):    # loops 14 times, creates 14 threads\n",
    "    start = i\n",
    "    end = i + 9\n",
    "    if start == 0:\n",
    "        start = 1 # There is no comic 0, so set it to 1.\n",
    "    pageThread = threading.Thread(target=downloadPage, args=(start, end))\n",
    "    pageThreads.append(pageThread)\n",
    "    pageThread.start()\n",
    "    \n",
    "# Wait for all threads to end.\n",
    "for pageThread in pageThreads:\n",
    "    pageThread.join()\n",
    "'''\n",
    "downloadPage()\n",
    "\n",
    "print('\\nDone.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
