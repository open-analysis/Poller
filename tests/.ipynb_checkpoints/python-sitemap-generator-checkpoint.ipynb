{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "https://www.cnn.com/\n",
      "('Threads: ', 1, ' Queue: ', 0, ' Checked: ', 0, ' Link Threads: ', 0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RunCrawler(Thread-18, started 11560)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-13-cfcfc80de7ac>\", line 207, in run\n",
      "    temp_type = temp_res.info().type\n",
      "AttributeError: 'HTTPMessage' object has no attribute 'type'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Checked: ', 0)\n",
      "Running XML Generator...\n",
      "('Sitemap saved in: ', 'sitemap.xml')\n",
      "1.0123145580291748\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "''' DOESN'T WORK '''\n",
    "\n",
    "# Python Sitemap Generator\n",
    "# Version: 0.2\n",
    "# Przemek Wiejak @ przemek@wiejak.us\n",
    "# GitHub: https://github.com/wiejakp/python-sitemap-generator\n",
    "\n",
    "import threading\n",
    "import urllib.parse\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import time\n",
    "import email.utils as eut\n",
    "\n",
    "from pprint import pprint\n",
    "from var_dump import var_dump\n",
    "from lxml import etree\n",
    "from urllib.parse import urlparse\n",
    "from lxml.html.soupparser import fromstring\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# sudo apt-get install python-beautifulsoup\n",
    "# sudo apt-get install python-pip\n",
    "# sudo apt-get install python3-pip\n",
    "# pip install setuptools\n",
    "# pip install var_dump\n",
    "\n",
    "queue = []\n",
    "checked = []\n",
    "threads = []\n",
    "types = ['text/html']\n",
    "\n",
    "link_threads = []\n",
    "\n",
    "#MaxThreads = 30\n",
    "#MaxSubThreads = 10\n",
    "\n",
    "# adjust to your liking\n",
    "MaxThreads = 10\n",
    "MaxSubThreads = 10\n",
    "\n",
    "# DEFINE YOUR URL\n",
    "InitialURL = 'https://www.cnn.com/'\n",
    "\n",
    "InitialURLInfo = urlparse(InitialURL)\n",
    "InitialURLLen = len(InitialURL.split('/'))\n",
    "InitialURLNetloc = InitialURLInfo.netloc\n",
    "InitialURLScheme = InitialURLInfo.scheme\n",
    "InitialURLBase = InitialURLScheme + '://' + InitialURLNetloc\n",
    "\n",
    "netloc_prefix_str = 'www.'\n",
    "netloc_prefix_len = len(netloc_prefix_str)\n",
    "\n",
    "run_ini = None\n",
    "run_end = None\n",
    "run_dif = None\n",
    "\n",
    "filename = 'sitemap.xml'\n",
    "\n",
    "request_headers = {\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Referer\": \"http://thewebsite.com\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "if InitialURLNetloc.startswith(netloc_prefix_str):\n",
    "    InitialURLNetloc = InitialURLNetloc[netloc_prefix_len:]\n",
    "\n",
    "class RunCrawler(threading.Thread):\n",
    "    # crawler start\n",
    "    run_ini = time.time()\n",
    "    run_end = None\n",
    "    run_dif = None\n",
    "\n",
    "    print(\"\")\n",
    "    print(InitialURL)\n",
    "    print(\"\")\n",
    "\n",
    "    def __init__(self, url):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        ProcessURL(url)\n",
    "\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        run = True\n",
    "\n",
    "        while run:\n",
    "            for index, thread in enumerate(threads):\n",
    "                if thread.isAlive() == False:\n",
    "                    del threads[index]\n",
    "\n",
    "            for index, thread in enumerate(link_threads):\n",
    "                if thread.isAlive() == False:\n",
    "                    del link_threads[index]\n",
    "\n",
    "            for index, obj in enumerate(queue):\n",
    "                if len(threads) < MaxThreads:\n",
    "                    thread = Crawl(index, obj)\n",
    "                    threads.append(thread)\n",
    "\n",
    "                    del queue[index]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if len(queue) == 0 and len(threads) == 0 and len(link_threads) == 0:\n",
    "                run = False\n",
    "\n",
    "                self.done()\n",
    "            else:\n",
    "                print(('Threads: ', len(threads), ' Queue: ', len(queue), ' Checked: ', len(checked), ' Link Threads: ', len(link_threads)))\n",
    "                time.sleep(1)\n",
    "\n",
    "    def done(self):\n",
    "        print(('Checked: ', len(checked)))\n",
    "        print ('Running XML Generator...')\n",
    "\n",
    "        # Running sitemap-generating script\n",
    "        Sitemap()\n",
    "\n",
    "        self.run_end = time.time()\n",
    "        self.run_dif = self.run_end - self.run_ini\n",
    "\n",
    "        print((self.run_dif))\n",
    "\n",
    "\n",
    "class Sitemap:\n",
    "    urlset = None\n",
    "    encoding = 'UTF-8'\n",
    "    xmlns = 'http://www.sitemaps.org/schemas/sitemap/0.9'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root()\n",
    "        self.children()\n",
    "        self.xml()\n",
    "\n",
    "    def done(self):\n",
    "        print ('Done')\n",
    "\n",
    "    def root(self):\n",
    "        self.urlset = etree.Element('urlset')\n",
    "        self.urlset.attrib['xmlns'] = self.xmlns\n",
    "\n",
    "    def children(self):\n",
    "        for index, obj in enumerate(checked):\n",
    "            url = etree.Element('url')\n",
    "            loc = etree.Element('loc')\n",
    "            lastmod = etree.Element('lastmod')\n",
    "            changefreq = etree.Element('changefreq')\n",
    "            priority = etree.Element('priority')\n",
    "\n",
    "            loc.text = obj['url']\n",
    "            lastmod_info =  None\n",
    "            lastmod_header = None\n",
    "            lastmod.text = None\n",
    "\n",
    "            if hasattr(obj['obj'], 'info'):\n",
    "                lastmod_info = obj['obj'].info()\n",
    "                lastmod_header = lastmod_info.getheader('Last-Modified')\n",
    "\n",
    "            # check if 'Last-Modified' header exists\n",
    "            if lastmod_header != None:\n",
    "                lastmod.text = FormatDate(lastmod_header)\n",
    "\n",
    "            if loc.text != None:\n",
    "                url.append(loc)\n",
    "\n",
    "            if lastmod.text != None:\n",
    "                url.append(lastmod)\n",
    "\n",
    "            if changefreq.text != None:\n",
    "                url.append(changefreq)\n",
    "\n",
    "            if priority.text != None:\n",
    "                url.append(priority)\n",
    "\n",
    "            self.urlset.append(url)\n",
    "\n",
    "    def xml(self):\n",
    "        f = open(filename, 'w')\n",
    "        print(etree.tostring(self.urlset, xml_declaration = True, encoding = self.encoding), file=f)\n",
    "        f.close()\n",
    "\n",
    "        print(('Sitemap saved in: ', filename))\n",
    "\n",
    "\n",
    "class Crawl(threading.Thread):\n",
    "    def __init__(self, index, obj):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        self.index = index\n",
    "        self.obj = obj\n",
    "\n",
    "        self.start()\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        temp_status = None\n",
    "        temp_object = None\n",
    "\n",
    "        try:\n",
    "            temp_req = urllib.request.Request(self.obj['url'], headers=request_headers)\n",
    "            temp_res = urllib.request.urlopen(temp_req)\n",
    "            temp_code = temp_res.getcode()\n",
    "            temp_type = temp_res.info().type\n",
    "\n",
    "            temp_status = temp_res.getcode()\n",
    "            temp_object = temp_res\n",
    "\n",
    "            if temp_code == 200:\n",
    "                if temp_type in types:\n",
    "                    temp_content = temp_res.read()\n",
    "\n",
    "                    #var_dump(temp_content)\n",
    "\n",
    "                    temp_data = fromstring(temp_content)\n",
    "\n",
    "                    temp_thread = threading.Thread(target=ParseThread, args=(self.obj['url'], temp_data))\n",
    "                    link_threads.append(temp_thread)\n",
    "                    temp_thread.start()\n",
    "\n",
    "        except urllib.error.HTTPError as e:\n",
    "            temp_status = e.code\n",
    "            pass\n",
    "\n",
    "        self.obj['obj'] = temp_object\n",
    "        self.obj['sta'] = temp_status\n",
    "\n",
    "        ProcessChecked(self.obj)\n",
    "\n",
    "\n",
    "def dump(obj):\n",
    "    '''return a printable representation of an object for debugging'''\n",
    "    newobj=obj\n",
    "\n",
    "    if '__dict__' in dir(obj):\n",
    "      newobj=obj.__dict__\n",
    "\n",
    "      if ' object at ' in str(obj) and '__type__' not in newobj:\n",
    "          newobj['__type__']=str(obj)\n",
    "\n",
    "          for attr in newobj:\n",
    "              newobj[attr]=dump(newobj[attr])\n",
    "\n",
    "    return newobj\n",
    "\n",
    "\n",
    "def FormatDate(datetime):\n",
    "    datearr = eut.parsedate(datetime)\n",
    "    date = None\n",
    "\n",
    "    try:\n",
    "        year = str(datearr[0])\n",
    "        month = str(datearr[1])\n",
    "        day = str(datearr[2])\n",
    "\n",
    "        if int(month) < 10:\n",
    "            month = '0' + month\n",
    "\n",
    "        if int(day) < 10:\n",
    "            day = '0' + day\n",
    "\n",
    "        date = year + '-' + month + '-' + day\n",
    "    except IndexError:\n",
    "        pprint(datearr)\n",
    "\n",
    "    return date\n",
    "\n",
    "\n",
    "def ParseThread(url, data):\n",
    "    temp_links = data.xpath('//a')\n",
    "\n",
    "    for temp_index, temp_link in enumerate(temp_links):\n",
    "        temp_attrs = temp_link.attrib\n",
    "\n",
    "        if 'href' in temp_attrs:\n",
    "            temp_url = temp_attrs.get('href')\n",
    "            temp_src = url\n",
    "            temp_value = temp_link.text\n",
    "            temp_url = temp_attrs.get('href')\n",
    "\n",
    "            path = JoinURL(temp_src, temp_url)\n",
    "\n",
    "            if path != False:\n",
    "                ProcessURL(path, temp_src)\n",
    "\n",
    "\n",
    "def JoinURL(src, url):\n",
    "    value = False\n",
    "\n",
    "    url_info = urlparse(url)\n",
    "    src_info = urlparse(src)\n",
    "\n",
    "    url_scheme = url_info.scheme\n",
    "    src_scheme = src_info.scheme\n",
    "\n",
    "    url_netloc = url_info.netloc\n",
    "    src_netloc = src_info.netloc\n",
    "\n",
    "    if src_netloc.startswith(netloc_prefix_str):\n",
    "        src_netloc = src_netloc[netloc_prefix_len:]\n",
    "\n",
    "    if url_netloc.startswith(netloc_prefix_str):\n",
    "        url_netloc = url_netloc[netloc_prefix_len:]\n",
    "\n",
    "    if url_netloc == '' or url_netloc == InitialURLNetloc:\n",
    "        url_path = url_info.path\n",
    "        src_path = src_info.path\n",
    "\n",
    "        src_new_path = urljoin(InitialURLBase, src_path)\n",
    "        url_new_path = urljoin(src_new_path, url_path)\n",
    "\n",
    "        path = urljoin(src_new_path, url_new_path)\n",
    "\n",
    "        #print path\n",
    "\n",
    "        value = path\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def ProcessURL(url, src = None, obj = None):\n",
    "    found = False\n",
    "    \n",
    "    for value in queue:\n",
    "        if value['url'] == url:\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    for value in checked:\n",
    "        if value['url'] == url:\n",
    "            found = True\n",
    "            break\n",
    "            \n",
    "    if found == False:\n",
    "        temp = {}\n",
    "        temp['url'] = url\n",
    "        temp['src'] = src\n",
    "        temp['obj'] = obj\n",
    "        temp['sta'] = None\n",
    "        \n",
    "        queue.append(temp)\n",
    "\n",
    "def ProcessChecked(obj):\n",
    "    found = False\n",
    "    \n",
    "    for item in checked:\n",
    "        if item['url'] == obj['url']:\n",
    "            found = True\n",
    "            break\n",
    "            \n",
    "    if found == False:\n",
    "        checked.append(obj)\n",
    "\n",
    "RunCrawler(InitialURL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
